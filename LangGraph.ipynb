{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9c7b00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +-----------+           \n",
      "        | __start__ |           \n",
      "        +-----------+           \n",
      "               *                \n",
      "               *                \n",
      "               *                \n",
      "          +-------+             \n",
      "          | start |             \n",
      "          +-------+             \n",
      "               *                \n",
      "               *                \n",
      "               *                \n",
      "         +----------+           \n",
      "         | classify |           \n",
      "         +----------+           \n",
      "          .        .            \n",
      "        ..          ..          \n",
      "       .              .         \n",
      "+---------+       +----------+  \n",
      "| explain |       | generate |  \n",
      "+---------+       +----------+  \n",
      "          *        *            \n",
      "           **    **             \n",
      "             *  *               \n",
      "           +-----+              \n",
      "           | end |              \n",
      "           +-----+              \n",
      "               *                \n",
      "               *                \n",
      "               *                \n",
      "         +---------+            \n",
      "         | __end__ |            \n",
      "         +---------+            \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b65f87",
   "metadata": {},
   "source": [
    "# Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7841a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9695e22a674ec0a61286464228f51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# langgraph_rag_gradio_app.py\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Literal, Dict\n",
    "import gradio as gr\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import traceback\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ====== STATE DEFINITION ======\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    user_input: str\n",
    "    intent: Optional[Literal[\"generate\", \"explain\"]] = None\n",
    "    retrieved_examples: Optional[List[Dict]] = None\n",
    "    formatted_prompt: Optional[str] = None\n",
    "    llm_output: Optional[str] = None\n",
    "    final_response: Optional[str] = None\n",
    "\n",
    "\n",
    "# ====== RAG SETUP ======\n",
    "\n",
    "with open(\"C:/Users/sief x/Desktop/Study Sessions/RAG/human-eval/data/HumanEval.jsonl/human-eval-v2-20210705.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "prompts = [item['prompt'] for item in data]\n",
    "task_ids = [item[\"task_id\"] for item in data]\n",
    "\n",
    "model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model_embed.encode(prompts, show_progress_bar=True).astype(\"float32\")\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "\n",
    "\n",
    "# ====== HELPERS ======\n",
    "\n",
    "def retrieve_similar_examples(query: str, k: int = 2) -> List[Dict]:\n",
    "    query_vec = model_embed.encode([query]).astype(\"float32\")\n",
    "    D, I = index.search(query_vec, k)\n",
    "    return [\n",
    "        {\n",
    "            \"task_id\": task_ids[idx],\n",
    "            \"prompt\": prompts[idx],\n",
    "            \"canonical_solution\": data[idx][\"canonical_solution\"]\n",
    "        }\n",
    "        for idx in I[0]\n",
    "    ]\n",
    "\n",
    "def generation_prompt(retrieved_context: List[Dict], user_prompt: str) -> str:\n",
    "    parts = [\"### EXAMPLES:\\n\"]\n",
    "    for context in retrieved_context:\n",
    "        parts.append(f\"# Task: {context['prompt'].strip()}\")\n",
    "        parts.append(f\"# Solution:\\n{context['canonical_solution'].strip()}\\n\")\n",
    "    parts.append(\"### NEW TASK:\")\n",
    "    parts.append(f\"# Task: {user_prompt.strip()}\")\n",
    "    parts.append(\"# Solution:\")\n",
    "    parts.append(\"def\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def truncate_at_delimiter(text, delimiter=\"# Task:\"):\n",
    "    return text.split(delimiter)[0].strip()\n",
    "\n",
    "def generate_code(prompt_text: str, max_tokens=128) -> str:\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    outputs = model_llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id or 50256\n",
    "    )\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    return truncate_at_delimiter(tokenizer.decode(generated_tokens, skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# ====== LANGGRAPH NODES ======\n",
    "\n",
    "def start_node(state: AgentState) -> AgentState:\n",
    "    return state  # Gradio input already captured\n",
    "def classify_intent(state: AgentState) -> AgentState:\n",
    "    text = state.user_input.lower()\n",
    "    if \"explain\" in text:\n",
    "        state.intent = \"explain\"\n",
    "    elif any(word in text for word in [\"generate\", \"write\", \"create\", \"make\", \"build\"]):\n",
    "        state.intent = \"generate\"\n",
    "    else:\n",
    "        raise ValueError(\"❌ Could not classify user intent. Please use keywords like 'generate' or 'explain'.\")\n",
    "    return state\n",
    "def route(state: AgentState) -> str:\n",
    "    return state.intent\n",
    "def explain_code(state: AgentState) -> AgentState:\n",
    "    code = state.user_input.replace(\"Explain this code:\", \"\").strip()\n",
    "    explanation = f\"This code defines a Python function or snippet. Without execution or full context, a static guess:\\n\\n{code}\\n\\n☝️ You can improve the result by adding more context.\"\n",
    "    state.final_response = explanation\n",
    "    return state\n",
    "def generate_code_node(state: AgentState) -> AgentState:\n",
    "    try:\n",
    "        retrieved = retrieve_similar_examples(state.user_input, k=2)\n",
    "        state.retrieved_examples = retrieved\n",
    "        formatted = generation_prompt(retrieved, state.user_input)\n",
    "        state.formatted_prompt = formatted\n",
    "        llm_result = generate_code(formatted)\n",
    "        state.llm_output = llm_result\n",
    "        state.final_response = \"def \" + llm_result\n",
    "    except Exception as e:\n",
    "        state.final_response = \"⚠️ Generation failed:\\n\" + str(e)\n",
    "    return state\n",
    "def end_node(state: AgentState) -> AgentState:\n",
    "    return state\n",
    "\n",
    "\n",
    "# ====== LANGGRAPH STRUCTURE ======\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"start\", start_node)\n",
    "graph.add_node(\"classify\", classify_intent)\n",
    "graph.add_node(\"generate\", generate_code_node)\n",
    "graph.add_node(\"explain\", explain_code)\n",
    "graph.add_node(\"end\", end_node)\n",
    "\n",
    "graph.set_entry_point(\"start\")\n",
    "graph.add_edge(\"start\", \"classify\")\n",
    "graph.add_conditional_edges(\"classify\", route, {\n",
    "    \"generate\": \"generate\",\n",
    "    \"explain\": \"explain\"\n",
    "})\n",
    "graph.add_edge(\"generate\", \"end\")\n",
    "graph.add_edge(\"explain\", \"end\")\n",
    "graph.set_finish_point(\"end\")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "\n",
    "# ====== GRADIO UI ======\n",
    "\n",
    "def gradio_interface(user_input: str) -> str:\n",
    "    try:\n",
    "        initial_state = AgentState(user_input=user_input)\n",
    "        final_state = app.invoke(initial_state)\n",
    "        return final_state[\"final_response\"]\n",
    "    except Exception as e:\n",
    "        return \"❌ Error:\\n\" + traceback.format_exc()\n",
    "\n",
    "gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(lines=4, placeholder=\"Enter 'Generate a function...' or 'Explain this code: ...'\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"🧠 LangGraph Code Assistant\",\n",
    "    description=\"Supports code generation and explanation using retrieval-augmented reasoning.\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b27a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_add_function():\n",
    "    input_text = \"Generate a function that adds two numbers\"\n",
    "    output = gradio_interface(input_text)\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Output:\", output)\n",
    "    assert \"def\" in output.lower()\n",
    "    assert \"+\" in output or \"add\" in output.lower()\n",
    "    print(\"✅ Passed: test_generate_add_function\")\n",
    "\n",
    "def test_generate_sorting_function():\n",
    "    input_text = \"Generate a function that sorts a list\"\n",
    "    output = gradio_interface(input_text)\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Output:\", output)\n",
    "    assert \"def\" in output.lower()\n",
    "    assert \"sort\" in output.lower()\n",
    "    print(\"✅ Passed: test_generate_sorting_function\")\n",
    "\n",
    "def test_explain_loop_code():\n",
    "    input_text = \"Explain this code: for i in range(5): print(i)\"\n",
    "    output = gradio_interface(input_text)\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Output:\", output)\n",
    "    assert any(keyword in output.lower() for keyword in [\"loop\", \"iterates\", \"range\", \"prints\"])\n",
    "    print(\"✅ Passed: test_explain_loop_code\")\n",
    "\n",
    "\n",
    "def test_invalid_input():\n",
    "    input_text = \"What's the weather in Cairo?\"\n",
    "    try:\n",
    "        output = gradio_interface(input_text)\n",
    "    except Exception as e:\n",
    "        print(\"✅ Properly raised error:\", str(e))\n",
    "        return\n",
    "    print(\"Output:\", output)\n",
    "    assert \"❌\" in output or \"could not classify\" in output.lower()\n",
    "    print(\"✅ Passed: test_invalid_input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1fbd0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Generate a function that adds two numbers\n",
      "Output: def add(x: int, y: int) -> int:\n",
      "    \"\"\"Add two numbers x and y\n",
      "    >>> add(2, 3)\n",
      "    5\n",
      "    >>> add(5, 7)\n",
      "    12\n",
      "    \"\"\"\n",
      "# Solution:\n",
      "    return x + y\n",
      "✅ Passed: test_generate_add_function\n",
      "Input: Generate a function that sorts a list\n",
      "Output: def sort_list_with_sort(l: list, sort_func: callable = None) -> list:\n",
      "    \"\"\"Sort a list.\n",
      "\n",
      "    Sort the elements of the list, using a given function.\n",
      "    If sort_func is None, it uses a default sort function.\n",
      "\n",
      "    >>> sort_list_with_sort([1, 2, 3, 4, 5, 6])\n",
      "    [1, 2, 3, 4, 5, 6]\n",
      "    >>> sort_list_with_sort([1, 2, 3, 4, 5, 6], lambda x: x*2\n",
      "✅ Passed: test_generate_sorting_function\n",
      "Input: Explain this code: for i in range(5): print(i)\n",
      "Output: This code defines a Python function or snippet. Without execution or full context, a static guess:\n",
      "\n",
      "for i in range(5): print(i)\n",
      "\n",
      "☝️ You can improve the result by adding more context.\n",
      "✅ Passed: test_explain_loop_code\n",
      "Output: ❌ Error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sief x\\AppData\\Local\\Temp\\ipykernel_11560\\2244737938.py\", line 151, in gradio_interface\n",
      "    final_state = app.invoke(initial_state)\n",
      "  File \"c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\langgraph\\pregel\\main.py\", line 3015, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\langgraph\\pregel\\main.py\", line 2642, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\langgraph\\pregel\\_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\langgraph\\pregel\\_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"c:\\Users\\sief x\\anaconda3\\envs\\tf_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\sief x\\AppData\\Local\\Temp\\ipykernel_11560\\2244737938.py\", line 98, in classify_intent\n",
      "    raise ValueError(\"❌ Could not classify user intent. Please use keywords like 'generate' or 'explain'.\")\n",
      "ValueError: ❌ Could not classify user intent. Please use keywords like 'generate' or 'explain'.\n",
      "\n",
      "✅ Passed: test_invalid_input\n"
     ]
    }
   ],
   "source": [
    "test_generate_add_function()\n",
    "test_generate_sorting_function()\n",
    "test_explain_loop_code()\n",
    "test_invalid_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0221ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
